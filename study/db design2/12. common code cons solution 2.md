* by caching, improve application common_code query
- network communication is reduced by caching

* common_code characteristics
1. data is very small: ~1000
2. change is very rare: sometimes in a week
3. query is very frequent: 100~10000 times per second
these characteristics make caching effective

* memory caching power

access time by storage type(approximate value)

| storage                         | real speed           | relative cost |
|---------------------------------|----------------------|---------------|
| L1 cache                        | 1ns                  | 1             |
| Memory(RAM)                     | 100ns                | 100           |
| SSD                             | 100us(100,000ns)     | 100,000       |
| network(in the same datacenter) | 500us(500,000ns)     | 500,000       |
| network(overseas)               | 150ms(150,000,000ns) | 150,000,000   |

insight
- find in memory: almost free
- find by network(db query): very expensive

* cache synchronization problem
- local memory cache is simple and fast, but it has a synchronization problem
- miss match between local memory and remote db

* try to solve a cache synchronization problem
- solution1: cache invalidation when common_code is updated
  - issue: multi server environment. between servers, how to invalidate cache?
- solution2: cache invalidation request to all servers
  - issue: it is complex
    - all servers is managed by a centralized server
    - when server is added or removed, server list is updated
    - how to handle the case when some request is failed?
    - if server count is large, hundreds of requests are required
- solution3: centralized cache like redis
  - issue: redis is also network communication 

| approach                     | pros                                | cons                                 |
|------------------------------|-------------------------------------|--------------------------------------|
| local memory cache           | very fast(no network communication) | multi server synchronization is hard |
| centralized cache(ex. redis) | synchronization is easy             | network communication is required    |


* optimized approach by using TTL
strategy using local cache and TTL has both pros and cons
core idea
1. common_code is cached in local memory(fast query)
2. set cache TTL(valid time) (ex. 1minute)
3. when TTL is expired, invalidate cache and query db
```java
// pseudo code
class CommonCodeCache {
    private Map<String, Map<String, String>> cache = new HashMap<>();
    private DataTime lastLoadedTime = new Date();
    private int TTL_SECONDS = 60; // 1 minute
    
    String getName(String groupCode, String code) {
        // check cache TTL
        if (isExpired()) {
            refresh();
        }
        return cache.get(groupCode).get(code);
    }
    
    boolean isExpired() {
        return (now() - lastLoadedTime) > TTL_SECONDS;
    }
    
    void refresh() {
        cache = loadFromDb();
        lastLoadedTime = now();
    }
}
```
- by using TTL, cache is refreshed automatically per TTL

* is 1 minute TTL enough?
- production environment can allow 1 minute enough
- when is a common_code updated?
  - new payment is added
  - display name typo is fixed
  - unused common_code is inactivated
  -> these can allow 1 minute
if 1 minute is not enough 
- TTL can be adjusted to 10 seconds or 30 seconds
- or centeralized cache like redis can be used

* performance effect calculation
scenario
- 10 users enter in 1 second
- each user request 20 items of list in 1 second
- each item require common_code name
- TTL is 60 seconds

case query db without cache(only common_code query)
- common_code query(1 second): 10 user * 20 items = 200 queries
- query in 60 seconds: 200 queries * 60 = 12,000 queries

case query db with cache(only common_code query)
- common_code query(1 second): 1query (1 time when TTL is expired)
- left 11,999 queries: query in memory(network communication is free)

|              | db query counts | decrease rate |
|--------------|-----------------|---------------|
| before cache | 12,000          |               |
| after cache  | 1               | 99.99%        |

* practice tips - don't reinvent the wheel
- for concurrency, memory leak prevention, use a cache library

* summary

| strategy                     | speed  | synchronization | complexity | recommendation |
|------------------------------|--------|-----------------|------------|----------------|
| no cache                     | slow   | always fresh    | low        | X              |
| local cache without TTL      | fast   | hard            | medium     | X              |
| centralized cache(ex. redis) | medium | easy            | high       | midium         |
| local cache + TTL            | fast   | automatically   | medium     | O              |

* local cache + TTL
- query in memory without network communication
- db synchronization is done automatically per TTL
- in a multiserver environment, complex synchronization logic is not required
- the implementation is relatively simple

* recommended TTL value
- normal case: 60 seconds
- case that fast synchronization is required: 10~30 seconds
- case that cache is used for a long time: over 5 minutes
- 