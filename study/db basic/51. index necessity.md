* index necessity

- full table scan
  - one by one, compare a row 
  - best: O(1)
  - worst: O(N)

* performance(response time) dependency
- server hardware(cpu, memory, disk i/o performance)
- environment setup
- size per single row
- system current stress

- approximate response time
  - suppose
    - normal enterprise server environment(based on ssd disk)
    - a single row size is about 1KB
  - 1. million rows
    - total data size: about 1GB
    - estimated response time: seconds
    - note: depend on disk i/o, fast if data is in memory(buffer cache)
  - 2. 10 million rows
    - total data size: about 10GB
    - estimated response time: seconds ~ 1 minute
    - note: disk i/o bottleneck occur in earnest
  - 3. 1 billion rows
    - total data size: about 100GB
    - estimated response time: over tens of seconds
    - note: if race conditions occur, hard to estimate time 
it is just references, various by environment
  - check performance test
  - actually handle multiple requests at the same time. it makes slower

* practice tips1
- user doesn't wait loading 
- response time is important

* practice tips2
- a full table scan is a very expensive operation
- solution
  - index usage: for prevention for full scan, a basic solution is index creation for frequent where clause condition
  - check Execution Plan
  - run heavy work in idle time: statistics/batch execution when users use less service    